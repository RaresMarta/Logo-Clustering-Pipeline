{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logo Similarity\n",
    "### Task\n",
    "Match and group websites by the **similarity of their logos**.\n",
    "### Steps to Achieve This\n",
    "✅ **1:** Get logo for each website as **Favicons**  \n",
    "✅ **2:** Extract **Image Features** (Convert each favicon into numerical data)  \n",
    "✅ **3:** Measure **Similarity** (Use cosine similarity)  \n",
    "✅ **4:** Apply **Clustering Algorithm** (DBSCAN)  \n",
    "✅ **5:** **Visualize the Groups** (Verify clustering results)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import hashlib\n",
    "import argparse\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from io import BytesIO\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_distances, euclidean_distances\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Constants & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FILE = \"logos.snappy.parquet\"\n",
    "FAVICON_CSV = \"favicons_fetched.csv\"\n",
    "TARGET_SIZE = (128, 128)\n",
    "MAX_WORKERS_FETCH = 15\n",
    "MAX_WORKERS_DOWNLOAD = 10\n",
    "\n",
    "# Fixed DBSCAN parameters\n",
    "DBSCAN_EPS = 0.2\n",
    "DBSCAN_MIN_SAMPLES = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load dataset\n",
    "Loads the dataset from a parquet file, removes duplicate domains, and performs initial data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data() -> pd.DataFrame:\n",
    "    \"\"\"Load dataset, remove duplicates, and filter domains by regex.\"\"\"\n",
    "    print(\"\\nLoading dataset...\")\n",
    "\n",
    "    df = pd.read_parquet(DATASET_FILE, engine=\"pyarrow\")\n",
    "    print(f\"Initial dataset size: {len(df)}\")\n",
    "\n",
    "    # Remove duplicates\n",
    "    dup_count = df['domain'].duplicated().sum()\n",
    "    df = df.drop_duplicates(subset=[\"domain\"])\n",
    "\n",
    "    print(f\"Removed {dup_count} duplicate domains.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Favicon Fetching\n",
    "Retrieves favicon URLs for each domain by first using Google's favicon API and then a direct access method; optionally caches the results in a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_image(response) -> bool:\n",
    "    \"\"\"Returns True if the response content is a valid image.\"\"\"\n",
    "    try:\n",
    "        Image.open(BytesIO(response.content))\n",
    "        return True\n",
    "    except UnidentifiedImageError:\n",
    "        return False\n",
    "\n",
    "def get_favicon(domain: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Try Google's favicon API first, then direct /favicon.ico.\n",
    "    Returns a URL if found, otherwise None.\n",
    "    \"\"\"\n",
    "    google_favicon = f\"https://www.google.com/s2/favicons?sz=128&domain={domain}\"\n",
    "    direct_favicon = f\"https://{domain}/favicon.ico\"\n",
    "    for url in [google_favicon, direct_favicon]:\n",
    "        try:\n",
    "            response = requests.get(url, timeout=4, stream=True)\n",
    "            time.sleep(0.1)\n",
    "            if response.status_code == 200 and is_valid_image(response):\n",
    "                return url\n",
    "        except requests.RequestException:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def fetch_favicon_urls(valid_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Retrieve favicon URLs, optionally cached in a CSV.\"\"\"\n",
    "    if os.path.exists(FAVICON_CSV):\n",
    "        print(\"\\nLoading previously fetched favicon URLs...\")\n",
    "        fav_df = pd.read_csv(FAVICON_CSV)\n",
    "    else:\n",
    "        print(\"\\nFetching favicon URLs in parallel...\")\n",
    "        with ThreadPoolExecutor(max_workers=MAX_WORKERS_FETCH) as executor:\n",
    "            valid_df[\"logo_url\"] = list(executor.map(get_favicon, valid_df[\"domain\"]))\n",
    "        fav_df = valid_df.dropna(subset=[\"logo_url\"]).copy()\n",
    "        print(f\"Favicons retrieved for {len(fav_df)} domains out of {len(valid_df)} validated domains.\")\n",
    "        fav_df.to_csv(FAVICON_CSV, index=False)\n",
    "    return fav_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Download Images (in memory only)\n",
    "Downloads favicon images in parallel using a thread pool and stores them in memory for subsequent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_favicon_image(logo_url: str) -> tuple[str, Image.Image | None]:\n",
    "    \"\"\"\n",
    "    Download an image from the provided favicon URL and store it in RAM.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(logo_url, timeout=5, stream=True)\n",
    "        response.raise_for_status()\n",
    "        if not is_valid_image(response):\n",
    "            return logo_url, None\n",
    "\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        # Composite transparency onto white if needed.\n",
    "        if image.mode in (\"P\", \"RGBA\"):\n",
    "            image = image.convert(\"RGBA\")\n",
    "            background = Image.new(\"RGBA\", image.size, (255, 255, 255, 255))\n",
    "            image = Image.alpha_composite(background, image).convert(\"RGB\")\n",
    "        else:\n",
    "            image = image.convert(\"RGB\")\n",
    "\n",
    "        image = image.resize(TARGET_SIZE, resample=Image.Resampling.LANCZOS)\n",
    "        return logo_url, image\n",
    "    except requests.RequestException:\n",
    "        return logo_url, None\n",
    "\n",
    "def download_favicon_images(fav_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Download favicon images in parallel (all in memory).\"\"\"\n",
    "    print(\"Downloading favicon images...\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS_DOWNLOAD) as executor:\n",
    "        results = list(executor.map(download_favicon_image, fav_df[\"logo_url\"]))\n",
    "\n",
    "    images_dict = {url: img for (url, img) in results if img is not None}\n",
    "    print(f\"Successfully downloaded {len(images_dict)} favicon images.\")\n",
    "    return images_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Remove Duplicates\n",
    "Computes MD5 hashes for each downloaded image to identify and remove duplicate icons, ensuring unique images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_md5(image: Image.Image) -> str:\n",
    "    \"\"\"Compute an MD5 hash for the in-memory image (PNG format).\"\"\"\n",
    "    buf = BytesIO()\n",
    "    image.save(buf, format=\"PNG\")\n",
    "    return hashlib.md5(buf.getvalue()).hexdigest()\n",
    "\n",
    "def remove_duplicate_icons(images_dict: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Remove duplicates in memory based on MD5 hash.\n",
    "    No icons are saved to disk.\n",
    "    \"\"\"\n",
    "    unique = {}\n",
    "    for url, img in images_dict.items():\n",
    "        md5_hash = compute_md5(img)\n",
    "        if md5_hash not in unique:\n",
    "            unique[md5_hash] = (url, img)\n",
    "\n",
    "    duplicates_removed = len(images_dict) - len(unique)\n",
    "    print(f\"Removed {duplicates_removed} duplicate icons in memory.\")\n",
    "    return {url: img for (url, img) in unique.values()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Model Loading & Feature Extraction\n",
    "Loads the specified deep learning model (either resnet50 or mobilenet_v2) and extracts deep features from each favicon image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name: str):\n",
    "    \"\"\"Load the specified model architecture, removing classification layer.\"\"\"\n",
    "    if model_name == \"resnet50\":\n",
    "        net = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "    elif model_name == \"mobilenet_v2\":\n",
    "        net = models.mobilenet_v2(weights=models.MobileNet_V2_Weights.IMAGENET1K_V1)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid model: {model_name}\")\n",
    "\n",
    "    net = torch.nn.Sequential(*list(net.children())[:-1])  # remove final layer\n",
    "    net.eval()\n",
    "    return net\n",
    "\n",
    "def extract_features_from_images(images_dict: dict, model_name: str) -> tuple[list, np.ndarray]:\n",
    "    \"\"\"Extract deep features from each image using the chosen model.\"\"\"\n",
    "    print(f\"\\nExtracting features using {model_name}...\")\n",
    "\n",
    "    model = load_model(model_name)\n",
    "    transform_pipeline = transforms.Compose([\n",
    "        transforms.Resize(TARGET_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    def extract_feats(image: Image.Image) -> np.ndarray | None:\n",
    "        try:\n",
    "            x = transform_pipeline(image).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                feats = model(x).flatten().numpy()\n",
    "            return feats\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    feat_map = {}\n",
    "    for url, img in images_dict.items():\n",
    "        feats = extract_feats(img)\n",
    "        if feats is not None:\n",
    "            feat_map[url] = feats\n",
    "\n",
    "    if not feat_map:\n",
    "        print(\"No features extracted. Exiting...\")\n",
    "        sys.exit()\n",
    "\n",
    "    feature_urls = list(feat_map.keys())\n",
    "    features_matrix = np.array(list(feat_map.values()))\n",
    "    print(f\"Extracted features from {len(feature_urls)} images.\")\n",
    "    return feature_urls, features_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. DBSCAN Clustering\n",
    "Clusters the extracted features using DBSCAN with cosine distance, grouping similar icons together and identifying noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_features(features_matrix: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cluster features using DBSCAN only (eps=0.2, min_samples=5).\n",
    "    \"\"\"\n",
    "    print(f\"\\nClustering with DBSCAN...\")\n",
    "    dist_matrix = cosine_distances(features_matrix)\n",
    "    db = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES, metric='precomputed')\n",
    "    labels = db.fit_predict(dist_matrix)\n",
    "    n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    print(f\"Found {n_clusters} clusters (label -1 indicates noise).\")\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Visualization & Sample Display\n",
    "Uses t-SNE for dimensionality reduction to generate a 2D visualization of clusters and saves both the t-SNE plot  \n",
    "and sample cluster plots to disk without displaying them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(features_matrix: np.ndarray, labels: np.ndarray, model_name: str, save_folder: str):\n",
    "    \"\"\"Reduce features to 2D using t-SNE, plot the clusters, and save the figure.\"\"\"\n",
    "    print(\"Reducing dimensions with t-SNE...\")\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=42, init='random', learning_rate=200.0)\n",
    "    X_embedded = tsne.fit_transform(features_matrix)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=labels, cmap='plasma', edgecolors='k')\n",
    "    plt.colorbar(scatter, label='Cluster ID')\n",
    "    plt.title(\"Favicon Clustering (DBSCAN)\")\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "    \n",
    "    # Build the filename: e.g. \"64_resnet50_tsne.png\"\n",
    "    filename = os.path.join(save_folder, f\"{TARGET_SIZE[0]}_{model_name}_tsne.png\")\n",
    "    plt.savefig(filename)\n",
    "    print(f\"t-SNE plot saved as {filename}\")\n",
    "    plt.close()\n",
    "\n",
    "def print_clusters_and_samples(labels: np.ndarray, feature_urls: list, images_dict: dict, model_name: str, save_folder: str, n_samples: int = 10):\n",
    "    \"\"\"Save each cluster's sample favicons in a horizontal layout to disk.\"\"\"\n",
    "    unique_clusters = set(labels)\n",
    "    unique_clusters.discard(-1)  # remove noise points\n",
    "\n",
    "    for cluster_id in unique_clusters:\n",
    "        cluster_urls = [u for u, lab in zip(feature_urls, labels) if lab == cluster_id]\n",
    "        print(f\"Cluster {cluster_id} has {len(cluster_urls)} icons.\")\n",
    "\n",
    "        if not cluster_urls:\n",
    "            continue\n",
    "\n",
    "        sample_urls = np.random.choice(cluster_urls, size=min(n_samples, len(cluster_urls)), replace=False)\n",
    "        fig, axes = plt.subplots(1, len(sample_urls), figsize=(15, 5))\n",
    "        for i, url in enumerate(sample_urls):\n",
    "            axes[i].imshow(images_dict[url])\n",
    "            axes[i].axis('off')\n",
    "        plt.suptitle(f\"Cluster {cluster_id} (showing {len(sample_urls)} of {len(cluster_urls)})\", fontsize=14)\n",
    "        \n",
    "        # Save the cluster sample plot with a filename e.g. \"64_resnet50_cluster_3.png\"\n",
    "        filename = os.path.join(save_folder, f\"{TARGET_SIZE[0]}_{model_name}_cluster_{cluster_id}.png\")\n",
    "        plt.savefig(filename)\n",
    "        print(f\"Cluster {cluster_id} sample plot saved as {filename}\")\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Argument Parsing & Main\n",
    "Parses command-line arguments (for model selection), sets up a directory structure for saving  \n",
    "plots, and orchestrates the overall pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(description=\"Favicon Clustering with DBSCAN (No Screen Plotting)\")\n",
    "    parser.add_argument(\"--model\", choices=[\"resnet50\", \"mobilenet_v2\"], default=\"mobilenet_v2\",\n",
    "                        help=\"Model architecture (default: mobilenet_v2)\")\n",
    "    return parser.parse_args()\n",
    "\n",
    "def main():\n",
    "    args = parse_arguments()\n",
    "\n",
    "    # Create a \"plots\" folder if it doesn't exist.\n",
    "    base_plots_folder = \"plots\"\n",
    "    if not os.path.exists(base_plots_folder):\n",
    "        os.makedirs(base_plots_folder)\n",
    "    \n",
    "    # Create a unique subfolder for this run.\n",
    "    # Using the structure: TARGET_SIZE_model (e.g. \"64_resnet50\")\n",
    "    base_run_folder = f\"{TARGET_SIZE[0]}_{args.model}\"\n",
    "    run_folder = os.path.join(base_plots_folder, base_run_folder)\n",
    "    counter = 1\n",
    "    while os.path.exists(run_folder):\n",
    "        run_folder = os.path.join(base_plots_folder, f\"{TARGET_SIZE[0]}_{args.model}_{counter}\")\n",
    "        counter += 1\n",
    "    os.makedirs(run_folder)\n",
    "    print(f\"Plots will be saved to: {run_folder}\")\n",
    "\n",
    "    # 1. Load & Clean Data\n",
    "    df = load_and_clean_data()\n",
    "\n",
    "    # 2. Fetch Favicon URLs\n",
    "    fav_df = fetch_favicon_urls(df)\n",
    "\n",
    "    # 3. Download images in memory\n",
    "    images_dict = download_favicon_images(fav_df)\n",
    "\n",
    "    # 4. Remove duplicates in memory\n",
    "    images_dict = remove_duplicate_icons(images_dict)\n",
    "\n",
    "    # 5. Extract features (user-chosen model)\n",
    "    feature_urls, features_matrix = extract_features_from_images(images_dict, args.model)\n",
    "\n",
    "    # 6. DBSCAN Clustering (using default cosine distance)\n",
    "    labels = cluster_features(features_matrix)\n",
    "\n",
    "    # 7. Visualization: t-SNE plot saved to disk\n",
    "    visualize_clusters(features_matrix, labels, args.model, run_folder)\n",
    "\n",
    "    # 8. Save cluster sample plots\n",
    "    print_clusters_and_samples(labels, feature_urls, images_dict, args.model, run_folder, n_samples=10)\n",
    "\n",
    "    print(\"Pipeline complete. All plots have been saved to the folder.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The pipeline efficiently processes your logo dataset by:\n",
    "\n",
    "- **Loading and Cleaning Data:** Reading the dataset from a parquet file and removing duplicate domains.\n",
    "- **Favicon Fetching and Image Downloading:** Retrieving favicon URLs (using Google's API or direct access), downloading images in parallel, and storing them in memory.\n",
    "- **Removing Duplicate Icons:** Using MD5 hashing to eliminate duplicate images.\n",
    "- **Feature Extraction:** Loading either the **resnet50** or **mobilenet_v2** model to extract deep features from each favicon.\n",
    "- **DBSCAN Clustering:** Grouping similar icons using DBSCAN with a cosine distance metric.\n",
    "- **Visualization:** Generating a t-SNE plot for a global view of the clusters and creating sample plots for each cluster.\n",
    "\n",
    "After experimenting with both **resnet50** and **mobilenet_v2**, I have concluded that **mobilenet_v2** does a better job at capturing the relevant visual features for this clustering task.\n",
    "\n",
    "### Plot Interpretation\n",
    "\n",
    "- **t-SNE Visualization:**  \n",
    "  This plot provides a 2D representation of the high-dimensional feature space. Logos that are visually similar tend to cluster together. Use this plot to get a global perspective on how your logos are grouped.\n",
    "\n",
    "- **Cluster Sample Plots:**  \n",
    "  Each cluster plot displays sample favicon images from that cluster. These plots help you inspect the composition of each cluster, confirming whether the grouping reflects meaningful visual similarities.\n",
    "\n",
    "### Folder Structure & Access\n",
    "\n",
    "All plots are saved directly to disk (with no on-screen display) in a structured manner under the **`plots`** directory:\n",
    "\n",
    "- **Main Folder:**  \n",
    "  `plots`\n",
    "\n",
    "- **Run-specific Subfolder:**  \n",
    "  Each run creates a subfolder named based on the target pixel size and selected model (e.g., `64_resnet50`). If a folder with that name already exists, a suffix (e.g., `_1`) is appended.\n",
    "\n",
    "- **Plot Files Inside the Subfolder:**  \n",
    "  - `64_resnet50_tsne.png` — The global t-SNE visualization.  \n",
    "  - `64_resnet50_cluster_X.png` — Individual cluster sample plots, where `X` represents the cluster number.\n",
    "\n",
    "You can review and interpret these plots by navigating to the corresponding subfolder in your application's directory.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
